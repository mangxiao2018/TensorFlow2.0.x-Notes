{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GradientTape是eager模式下计算梯度用的\n",
    "- watch(tensor)\n",
    "\n",
    "作用：确保某个tensor被tape追踪\n",
    "\n",
    "参数:tensor：一个Tensor或者一个Tensor列表\n",
    "\n",
    "- gradient(target, sources)\n",
    "\n",
    "作用：根据tape上面的上下文来计算某个或者某些tensor的梯度参数\n",
    "\n",
    "target:被微分的Tensor或Tensor列表，你可以理角为经过某个函数之后的值\n",
    "\n",
    "sources：Tensors或者Variables列表（当然可以只有一个值），你可以理解为函数的某个变量\n",
    "\n",
    "返回：\n",
    "\n",
    "一个列表表示各个变理的梯度值，和source中的变量列表一一对应，表明这个变量的梯度。\n",
    "\n",
    "下面的例子可中的梯度计算部分可以更直观的理解这个函数的用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    y = x * x\n",
    "    dy_dx = g.gradient(y, x) #y` = 2*x = 2*3 =6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5, shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例1、模型自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型（神经网络的前向传播）->定义损失函数->定义优化函数->定义tape->模型得到预测值->前向传播得到loss->反向传播->用优化函数将计算出来的梯度更新到变量上面去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu') # 隐藏层\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes) #输出层\n",
    "    def call(self, inputs):\n",
    "        # 定义前向传播\n",
    "        # 使用在__init__定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# 10 分类问题\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(data)\n",
    "    loss = loss_object(labels, predictions)\n",
    "\n",
    "gradients = tape.gradient(loss, model.trainable_variables) # 求梯度\n",
    "\n",
    "optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'my_model/dense/kernel:0' shape=(32, 32) dtype=float32, numpy=\n",
       " array([[-0.13878939, -0.01602063, -0.13419615, ..., -0.2536759 ,\n",
       "          0.12140706,  0.14884247],\n",
       "        [-0.2123425 ,  0.02857339,  0.1192424 , ..., -0.15994148,\n",
       "         -0.09618257,  0.21944065],\n",
       "        [-0.05170468, -0.19856381, -0.24476491, ...,  0.17599663,\n",
       "         -0.15568767, -0.30255696],\n",
       "        ...,\n",
       "        [-0.0309853 ,  0.14540717,  0.16526942, ..., -0.22511657,\n",
       "          0.10425398,  0.24007842],\n",
       "        [ 0.18376976,  0.11973252, -0.21223587, ..., -0.01269613,\n",
       "          0.0319657 ,  0.20275332],\n",
       "        [ 0.16396794,  0.17821506, -0.29619327, ..., -0.28940034,\n",
       "         -0.11030591,  0.16110216]], dtype=float32)>,\n",
       " <tf.Variable 'my_model/dense/bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.00099989,  0.00100002,  0.00099932,  0.00099974,  0.00100001,\n",
       "        -0.001     , -0.00100001, -0.00099982,  0.00099991, -0.00099999,\n",
       "        -0.00099985,  0.00099865, -0.00100002, -0.001     ,  0.00099987,\n",
       "         0.00100002, -0.00099977, -0.00100001, -0.00100002, -0.00099995,\n",
       "         0.00098825,  0.        , -0.00099609, -0.00100002,  0.00099994,\n",
       "        -0.00099909, -0.00099987, -0.00099972,  0.001     ,  0.00099779,\n",
       "        -0.00100002,  0.00099997], dtype=float32)>,\n",
       " <tf.Variable 'my_model/dense_1/kernel:0' shape=(32, 10) dtype=float32, numpy=\n",
       " array([[ 2.85268366e-01, -9.58009157e-03, -3.53722632e-01,\n",
       "          2.75698274e-01,  1.89378962e-01, -2.78041452e-01,\n",
       "          2.12257370e-01, -8.83008838e-02, -7.62039125e-02,\n",
       "         -2.57992446e-01],\n",
       "        [ 3.40262771e-01,  2.39868939e-01,  1.24085009e-01,\n",
       "         -3.42657238e-01,  3.17026317e-01,  3.32877994e-01,\n",
       "          1.13778472e-01,  1.76738501e-02, -3.14770907e-01,\n",
       "          1.42881751e-01],\n",
       "        [-1.84595957e-01, -3.39102179e-01, -2.33196199e-01,\n",
       "          3.61478448e-01,  1.50997058e-01, -2.76107311e-01,\n",
       "          3.16241622e-01, -1.74486011e-01,  1.53005674e-01,\n",
       "          3.15248162e-01],\n",
       "        [ 1.10279925e-01,  3.76334339e-02,  2.49364108e-01,\n",
       "          3.05104315e-01, -1.57571454e-02,  1.79679945e-01,\n",
       "          1.15813181e-01, -5.17131984e-02,  1.07746013e-01,\n",
       "          2.22162277e-01],\n",
       "        [ 2.46164903e-01,  1.89328313e-01, -1.19167842e-01,\n",
       "          4.37537655e-02,  4.42143343e-02, -2.71074567e-02,\n",
       "         -2.16932893e-01,  2.39841267e-01,  1.35265350e-01,\n",
       "          1.57017931e-02],\n",
       "        [ 2.89466083e-02, -7.99586922e-02,  2.03431159e-01,\n",
       "         -3.07824492e-01,  2.75977314e-01, -3.64023209e-01,\n",
       "          3.33236217e-01, -2.65994042e-01, -2.44215712e-01,\n",
       "         -1.29045576e-01],\n",
       "        [-1.96502328e-01, -2.89786085e-02, -2.29277045e-01,\n",
       "          2.05337778e-02, -2.17267126e-01,  8.00684094e-02,\n",
       "         -2.74613202e-01,  2.02358156e-01,  2.19694838e-01,\n",
       "         -3.23211819e-01],\n",
       "        [-2.34405279e-01, -4.27613705e-02, -8.73935148e-02,\n",
       "         -3.09067458e-01,  2.22785965e-01, -3.52900773e-01,\n",
       "          3.47434402e-01,  3.14065695e-01,  4.26773094e-02,\n",
       "         -3.00137848e-01],\n",
       "        [-2.46324122e-01,  2.46829540e-01,  8.37617815e-02,\n",
       "          1.60258502e-01, -1.10391248e-02,  2.07779899e-01,\n",
       "         -1.59765422e-01,  1.00919783e-01, -7.95323476e-02,\n",
       "         -1.52436286e-01],\n",
       "        [-2.58657753e-01,  2.18470082e-01, -3.41436654e-01,\n",
       "          3.73978466e-02,  4.36446257e-02, -6.05726428e-02,\n",
       "         -4.80345972e-02, -1.45660847e-01,  2.67709076e-01,\n",
       "         -7.11152405e-02],\n",
       "        [ 1.53015688e-01, -3.73458445e-01, -1.79996863e-01,\n",
       "         -3.00399095e-01,  3.33429694e-01, -1.21598840e-01,\n",
       "          1.30798310e-01,  2.02931315e-01,  2.28381958e-02,\n",
       "          1.23696111e-01],\n",
       "        [ 1.29043683e-01,  1.13949604e-01, -1.36170506e-01,\n",
       "          3.40380192e-01,  3.27547967e-01, -3.02081257e-01,\n",
       "          8.33942518e-02, -3.00181836e-01,  6.26810640e-03,\n",
       "         -1.77171707e-01],\n",
       "        [ 2.03502357e-01, -1.40697449e-01, -3.07867855e-01,\n",
       "         -7.93234929e-02, -2.32075587e-01, -2.40325153e-01,\n",
       "          3.30336466e-02,  1.06625862e-01, -2.57518142e-01,\n",
       "          2.27104783e-01],\n",
       "        [-7.01441541e-02, -9.54829231e-02, -1.84437737e-01,\n",
       "         -1.75460964e-01, -2.01061085e-01, -2.76906431e-01,\n",
       "         -2.63103992e-01,  2.28676111e-01, -9.37324688e-02,\n",
       "          1.52069241e-01],\n",
       "        [ 1.08267099e-01, -1.57981545e-01,  2.50347376e-01,\n",
       "          2.23158777e-01, -3.99669819e-03, -1.95862845e-01,\n",
       "          1.30719841e-01, -3.61959159e-01,  3.59594882e-01,\n",
       "         -2.99443007e-01],\n",
       "        [ 3.00160140e-01, -6.69632480e-02,  3.40659529e-01,\n",
       "          9.32548419e-02,  9.50399190e-02,  4.44639400e-02,\n",
       "         -1.89563315e-02,  2.10257262e-01, -2.88315624e-01,\n",
       "          2.73409337e-01],\n",
       "        [ 5.36490418e-02, -3.26985598e-01, -2.61588275e-01,\n",
       "         -5.18561341e-04, -3.29121619e-01,  1.19139459e-02,\n",
       "         -8.14592559e-03,  4.22951058e-02,  1.14360154e-01,\n",
       "         -1.00781158e-01],\n",
       "        [ 2.77019627e-02, -7.98508599e-02,  6.40893588e-04,\n",
       "         -2.69676801e-02, -2.99030095e-01,  1.19693153e-01,\n",
       "         -2.12648615e-01,  1.81430146e-01, -2.55305525e-02,\n",
       "         -1.77779108e-01],\n",
       "        [-1.15647644e-01, -3.54540259e-01,  6.65997639e-02,\n",
       "          1.68516815e-01, -1.29992038e-01,  3.64246726e-01,\n",
       "          2.21672535e-01, -2.90219069e-01, -2.95702040e-01,\n",
       "         -1.97106048e-01],\n",
       "        [-3.53649050e-01,  2.90814906e-01,  9.99778509e-02,\n",
       "         -2.25274056e-01, -1.93655908e-01, -3.41380239e-01,\n",
       "          3.32545608e-01, -2.09800079e-01, -3.20347585e-02,\n",
       "          2.38182321e-01],\n",
       "        [-2.35365909e-02, -2.39718780e-01,  8.14939439e-02,\n",
       "         -1.93147451e-01, -3.76189202e-01, -1.99640375e-02,\n",
       "          1.94234550e-01, -2.38278471e-02,  3.67505431e-01,\n",
       "          3.38293910e-01],\n",
       "        [-5.12667000e-02, -3.27399373e-02, -2.90274322e-02,\n",
       "         -2.12293401e-01,  7.26595223e-02, -2.00879857e-01,\n",
       "         -3.54255140e-02, -2.20801935e-01,  3.59130949e-01,\n",
       "          2.02359557e-02],\n",
       "        [ 3.52052033e-01, -2.30304286e-01, -3.11725557e-01,\n",
       "          2.85224974e-01, -3.00172269e-01,  1.97640255e-01,\n",
       "          1.20142978e-02, -1.33869588e-01,  1.62149802e-01,\n",
       "         -2.44186725e-02],\n",
       "        [-2.95095742e-01, -2.71006897e-02,  1.49084017e-01,\n",
       "          6.19134791e-02,  3.64063308e-02, -2.55146593e-01,\n",
       "         -3.59460503e-01,  2.27561459e-01, -2.74512053e-01,\n",
       "         -9.51855332e-02],\n",
       "        [ 3.38630140e-01,  1.49132177e-01, -5.26531637e-02,\n",
       "          2.60850698e-01,  1.33921966e-01, -1.58903271e-01,\n",
       "          1.04056522e-02, -2.31740296e-01, -2.32311875e-01,\n",
       "          2.43070453e-01],\n",
       "        [-1.44813552e-01, -3.41966420e-01, -2.38314852e-01,\n",
       "         -2.50541139e-02, -2.54265934e-01, -1.50619552e-01,\n",
       "         -3.04896861e-01,  2.89076567e-01,  3.32127005e-01,\n",
       "          4.59502637e-02],\n",
       "        [ 3.70302439e-01, -2.36385494e-01,  1.75055757e-01,\n",
       "         -3.38840872e-01, -2.33790755e-01,  7.08400160e-02,\n",
       "          3.04329753e-01, -6.94773570e-02,  2.70280745e-02,\n",
       "          4.87594455e-02],\n",
       "        [-2.80137628e-01, -3.64739388e-01,  3.27429295e-01,\n",
       "         -1.38749257e-01, -3.14847678e-01, -3.31195056e-01,\n",
       "          1.40200078e-01,  2.22351208e-01, -3.61445476e-04,\n",
       "         -2.99861938e-01],\n",
       "        [ 2.56990165e-01,  1.49057060e-01,  1.16226643e-01,\n",
       "         -1.71132505e-01, -2.94503927e-01,  1.89034417e-01,\n",
       "          5.93649335e-02,  3.37442547e-01, -2.93316931e-01,\n",
       "          2.97973663e-01],\n",
       "        [-2.64599651e-01, -3.47500682e-01,  1.36497587e-01,\n",
       "         -2.19704002e-01,  3.78345966e-01,  3.05663317e-01,\n",
       "         -6.03494868e-02,  1.69439822e-01, -2.71019965e-01,\n",
       "          2.07529426e-01],\n",
       "        [-1.33952901e-01, -2.22792432e-01, -3.18410605e-01,\n",
       "         -3.14372689e-01, -3.03430289e-01,  1.21409567e-02,\n",
       "         -3.74946035e-02,  2.56241024e-01,  2.37467766e-01,\n",
       "         -3.37881386e-01],\n",
       "        [ 1.76822454e-01,  3.13525021e-01, -2.59287596e-01,\n",
       "          1.02698520e-01, -2.14992210e-01,  3.60670120e-01,\n",
       "          1.19574286e-01, -2.36352682e-01, -2.84241915e-01,\n",
       "          2.06901655e-01]], dtype=float32)>,\n",
       " <tf.Variable 'my_model/dense_1/bias:0' shape=(10,) dtype=float32, numpy=\n",
       " array([0.00100002, 0.00100002, 0.00100002, 0.00100002, 0.00100002,\n",
       "        0.00100002, 0.00100002, 0.00100002, 0.00100002, 0.00100002],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例2、GradientType自定义训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='MyModel')\n",
    "        self.num_classes = num_classes\n",
    "        # 自定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # 定义前向传播\n",
    "        # 使用在'__init__'定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(num_classes = 10)\n",
    "# Instantiate an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "# Prepare the trainint dataset\n",
    "batch_size =64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch\n",
    "# batch_size\n",
    "# tape 求梯度，梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "WARNING:tensorflow:Layer MyModel is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\zhangyanqing\\.conda\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Training loss (for one batch) at step 0: 39.04924011230469\n",
      "Seen so far: 64 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 38.56948471069336\n",
      "Seen so far: 64 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 35.455162048339844\n",
      "Seen so far: 64 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    # 遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # 打开GradientType以记录正向传递期间运行的操作，这将启用自动区分。\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 运行该模型的前向传播。模型应用于其输入的操作将记录在GradientTape上。\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            # 计算这个minibatch的损失值\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        # 使用GradientTape自动获取可训练变量对于损失的梯度。    \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        # 通过更新变量的值来最大程度地减少损失，从而执行梯度下降的一步。\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        # 每200 batches打印一次\n",
    "        if step % 200 == 0:\n",
    "            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例3、使用GradientTape自定义训练模型进阶(加入评估函数)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们将metric添加到组合中，下面可以从头开始编写的训练循环中随时使用内置指标(或编写的自定义指标)。流程如下：\n",
    "\n",
    "- 在循环开始时初始化metrics\n",
    "- metric.update_state():每batch之后更新\n",
    "- metric.result():需要显示metric的当前值时调用\n",
    "- metric.reset_states():需要清除metrics状态时重置(通常在每个epoch的结尾)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # 定义前向传播\n",
    "        # 使用 __init__定义的层\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = np.random.random((1000,32))\n",
    "y_train = np.random.random((1000,10))\n",
    "x_val = np.random.random((200,32))\n",
    "y_val = np.random.random((200,10))\n",
    "x_test = np.random.random((200,32))\n",
    "y_test = np.random.random((200,10))\n",
    "\n",
    "#优化器\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "#损失函数\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "#准备metrics函数\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "#准备训练数据集\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "#准备测试数据集\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行几个epoch运行训练循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.1875\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.03125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.109375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.05000000074505806\n",
      "Validation acc :0.125 \n",
      "Start of epoch 1\n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.0625\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.125\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.109375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.046875\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.0625\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.15625\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.171875\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.10000000149011612\n",
      "Validation acc :0.11999999731779099 \n",
      "Start of epoch 2\n",
      "Training acc over epoch :0.140625\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.125\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.0\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.125\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.140625\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.046875\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.046875\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.15625\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.09375\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.078125\n",
      "Validation acc :0.11999999731779099 \n",
      "Training acc over epoch :0.109375\n",
      "Validation acc :0.125 \n",
      "Training acc over epoch :0.02500000037252903\n",
      "Validation acc :0.12999999523162842 \n"
     ]
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    #遍历数据集的batch_size\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # 一个batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_weights))\n",
    "        \n",
    "        #更新训练集的metrics\n",
    "        train_acc_metric(y_batch_train,logits)\n",
    "        \n",
    "        #在每个epoch结束时显示metrics\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print('Training acc over epoch :%s' % (float(train_acc),))\n",
    "        \n",
    "        #在每个epoch结束时重置训练指标\n",
    "        train_acc_metric.reset_states()\n",
    "        \n",
    "        #在每个epoch结束时运行一个验证集\n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val)\n",
    "            #更新验证集metrics\n",
    "            val_acc_metric(y_batch_val, val_logits)\n",
    "        val_acc = val_acc_metric.result()\n",
    "        print('Validation acc :%s ' % (float(val_acc),))\n",
    "        val_acc_metric.reset_states()\n",
    "        \n",
    "        #显示测试集\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
