{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结合focal loss 函数讲解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc0\n",
      "1.18.4\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load(\"mnist.npz\")\n",
    "x_train, y_train, x_test, y_test = mnist['x_train'], mnist['y_train'], mnist['x_test'], mnist['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test /255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
       "        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
       "        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
       "        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,\n",
       "        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,\n",
       "        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31372549, 0.61176471,\n",
       "        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,\n",
       "        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,\n",
       "        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764706, 0.94117647,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
       "        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,\n",
       "        0.25098039, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882353,\n",
       "        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568627,\n",
       "        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333333,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,\n",
       "        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=2,\n",
    "    ncols=5,\n",
    "    sharex=True,\n",
    "    sharey=True,)\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = x_train[y_train == i][10].reshape(28,28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "    \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "y_train = tf.one_hot(y_train, depth=10)\n",
    "y_test = tf.one_hot(y_test, depth=10)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')\n",
    "        self.d2 = Dense(10, activation='softmax')\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FocalLoss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_pred = tf.nn.softmax(y_pred,axis=-1)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1.0)\n",
    "        \n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        loss = - y_true * tf.math.pow(1 - y_pred, gamma) * tf.math.log(y_pred)\n",
    "        \n",
    "        loss = tf.math.reduce_sum(loss,axis=1)\n",
    "        return loss\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "\n",
    "# loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss_object = FocalLoss(gamma=2.0,alpha=0.25)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, prdictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655757904052734 , Test Accuracy: 8.548571586608887\n",
      "Epoch 2, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655765056610107 , Test Accuracy: 8.548571586608887\n",
      "Epoch 3, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655729293823242 , Test Accuracy: 8.548571586608887\n",
      "Epoch 4, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655749559402466 , Test Accuracy: 8.548571586608887\n",
      "Epoch 5, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655749559402466 , Test Accuracy: 8.548571586608887\n",
      "Epoch 6, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655747175216675 , Test Accuracy: 8.548571586608887\n",
      "Epoch 7, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655750751495361 , Test Accuracy: 8.548571586608887\n",
      "Epoch 8, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.865573763847351 , Test Accuracy: 8.548571586608887\n",
      "Epoch 9, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655730485916138 , Test Accuracy: 8.548571586608887\n",
      "Epoch 10, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655765056610107 , Test Accuracy: 8.548571586608887\n",
      "Epoch 11, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655749559402466 , Test Accuracy: 8.548571586608887\n",
      "Epoch 12, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655751943588257 , Test Accuracy: 8.548571586608887\n",
      "Epoch 13, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.865574598312378 , Test Accuracy: 8.548571586608887\n",
      "Epoch 14, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655763864517212 , Test Accuracy: 8.548571586608887\n",
      "Epoch 15, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655756711959839 , Test Accuracy: 8.548571586608887\n",
      "Epoch 16, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655741214752197 , Test Accuracy: 8.548571586608887\n",
      "Epoch 17, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655738830566406 , Test Accuracy: 8.548571586608887\n",
      "Epoch 18, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655751943588257 , Test Accuracy: 8.548571586608887\n",
      "Epoch 19, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655754327774048 , Test Accuracy: 8.548571586608887\n",
      "Epoch 20, Loss: 0.0, Accuracy: 0.0 , Test Loss: 1.8655738830566406 , Test Accuracy: 8.548571586608887\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    # 在下一个epoch开始时，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "    for images, labels in train_ds:\n",
    "        test_step(images, labels)\n",
    "    \n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    \n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {} , Test Loss: {} , Test Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result() * 100,\n",
    "                         test_loss.result(),\n",
    "                         test_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
